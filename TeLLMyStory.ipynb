{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# TeLLMyStory\n",
        "## BLUMET Thomas HALVICK Thomas LETRUNG Ethan 5A INFO Polytech Lyon - M2IA\n",
        "### Notebook use on Azure AI for running the training and evaluate the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'3.10.14 (main, May  6 2024, 19:42:50) [GCC 11.2.0]'"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Check your python version\n",
        "import sys\n",
        "sys.version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/bin/bash: 4.36.2: No such file or directory\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "# install the transformer library from Hugging Face\n",
        "%pip install transformers<4.36.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.45.1-py3-none-manylinux_2_24_x86_64.whl.metadata (5.8 kB)\n",
            "Collecting evaluate\n",
            "  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: torch~=2.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from bitsandbytes) (2.5.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from bitsandbytes) (1.23.5)\n",
            "Requirement already satisfied: datasets>=2.0.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from evaluate) (3.2.0)\n",
            "Requirement already satisfied: dill in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from evaluate) (0.3.8)\n",
            "Requirement already satisfied: pandas in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from evaluate) (1.3.5)\n",
            "Requirement already satisfied: requests>=2.19.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from evaluate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from evaluate) (4.66.5)\n",
            "Requirement already satisfied: xxhash in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from evaluate) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from evaluate) (0.70.16)\n",
            "Requirement already satisfied: fsspec>=2021.05.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from fsspec[http]>=2021.05.0->evaluate) (2023.10.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from evaluate) (0.27.1)\n",
            "Requirement already satisfied: packaging in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from evaluate) (24.1)\n",
            "Requirement already satisfied: filelock in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.16.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (17.0.0)\n",
            "Requirement already satisfied: aiohttp in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.11.11)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2024.8.30)\n",
            "Requirement already satisfied: networkx in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch~=2.0->bitsandbytes) (3.3)\n",
            "Requirement already satisfied: jinja2 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch~=2.0->bitsandbytes) (3.1.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch~=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch~=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch~=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch~=2.0->bitsandbytes) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch~=2.0->bitsandbytes) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch~=2.0->bitsandbytes) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch~=2.0->bitsandbytes) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch~=2.0->bitsandbytes) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch~=2.0->bitsandbytes) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch~=2.0->bitsandbytes) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch~=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch~=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch~=2.0->bitsandbytes) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch~=2.0->bitsandbytes) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from sympy==1.13.1->torch~=2.0->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from pandas->evaluate) (2.9.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from pandas->evaluate) (2024.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (5.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.18.3)\n",
            "Requirement already satisfied: six>=1.5 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from python-dateutil>=2.7.3->pandas->evaluate) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from jinja2->torch~=2.0->bitsandbytes) (2.1.5)\n",
            "Downloading bitsandbytes-0.45.1-py3-none-manylinux_2_24_x86_64.whl (69.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.7/69.7 MB\u001b[0m \u001b[31m52.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
            "Installing collected packages: bitsandbytes, evaluate\n",
            "Successfully installed bitsandbytes-0.45.1 evaluate-0.4.3\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "# install other libraries use in the project\n",
        "%pip install -q -U accelerate optimum \n",
        "%pip install bitsandbytes evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: datasets in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (3.2.0)\n",
            "Requirement already satisfied: filelock in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from datasets) (1.23.5)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from datasets) (1.3.5)\n",
            "Requirement already satisfied: requests>=2.32.2 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from datasets) (4.66.5)\n",
            "Requirement already satisfied: xxhash in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2023.10.0)\n",
            "Requirement already satisfied: aiohttp in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from datasets) (3.11.11)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from datasets) (0.27.1)\n",
            "Requirement already satisfied: packaging in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from datasets) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from aiohttp->datasets) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from aiohttp->datasets) (5.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from pandas->datasets) (2.9.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.16.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "#install library for the use of Bookcorpus and Roc-Stories\n",
        "%pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Name: datasets\n",
            "Version: 3.2.0\n",
            "Summary: HuggingFace community-driven open-source library of datasets\n",
            "Home-page: https://github.com/huggingface/datasets\n",
            "Author: HuggingFace Inc.\n",
            "Author-email: thomas@huggingface.co\n",
            "License: Apache 2.0\n",
            "Location: /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages\n",
            "Requires: aiohttp, dill, filelock, fsspec, huggingface-hub, multiprocess, numpy, packaging, pandas, pyarrow, pyyaml, requests, tqdm, xxhash\n",
            "Required-by: evaluate, optimum\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "# show the version of library\n",
        "%pip show datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://huggingface.github.io/autogptq-index/whl/cu117/\n",
            "Collecting auto-gptq\n",
            "  Downloading auto_gptq-0.7.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
            "Requirement already satisfied: accelerate>=0.26.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from auto-gptq) (1.3.0)\n",
            "Requirement already satisfied: datasets in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from auto-gptq) (3.2.0)\n",
            "Collecting sentencepiece (from auto-gptq)\n",
            "  Downloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
            "Requirement already satisfied: numpy in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from auto-gptq) (1.23.5)\n",
            "Collecting rouge (from auto-gptq)\n",
            "  Downloading rouge-1.0.1-py3-none-any.whl.metadata (4.1 kB)\n",
            "Collecting gekko (from auto-gptq)\n",
            "  Downloading gekko-1.2.1-py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: torch>=1.13.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from auto-gptq) (2.5.1)\n",
            "Requirement already satisfied: safetensors in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from auto-gptq) (0.5.2)\n",
            "Requirement already satisfied: transformers>=4.31.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from auto-gptq) (4.48.1)\n",
            "Collecting peft>=0.5.0 (from auto-gptq)\n",
            "  Downloading peft-0.14.0-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: tqdm in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from auto-gptq) (4.66.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from accelerate>=0.26.0->auto-gptq) (24.1)\n",
            "Requirement already satisfied: psutil in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from accelerate>=0.26.0->auto-gptq) (6.0.0)\n",
            "Requirement already satisfied: pyyaml in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from accelerate>=0.26.0->auto-gptq) (6.0.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from accelerate>=0.26.0->auto-gptq) (0.27.1)\n",
            "Requirement already satisfied: filelock in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq) (4.12.2)\n",
            "Requirement already satisfied: networkx in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq) (3.3)\n",
            "Requirement already satisfied: jinja2 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq) (2023.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from sympy==1.13.1->torch>=1.13.0->auto-gptq) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from transformers>=4.31.0->auto-gptq) (2024.11.6)\n",
            "Requirement already satisfied: requests in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from transformers>=4.31.0->auto-gptq) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from transformers>=4.31.0->auto-gptq) (0.21.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from datasets->auto-gptq) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from datasets->auto-gptq) (0.3.8)\n",
            "Requirement already satisfied: pandas in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from datasets->auto-gptq) (1.3.5)\n",
            "Requirement already satisfied: xxhash in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from datasets->auto-gptq) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from datasets->auto-gptq) (0.70.16)\n",
            "Requirement already satisfied: aiohttp in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from datasets->auto-gptq) (3.11.11)\n",
            "Requirement already satisfied: six in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from rouge->auto-gptq) (1.16.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from aiohttp->datasets->auto-gptq) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from aiohttp->datasets->auto-gptq) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from aiohttp->datasets->auto-gptq) (5.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from aiohttp->datasets->auto-gptq) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from aiohttp->datasets->auto-gptq) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from aiohttp->datasets->auto-gptq) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from aiohttp->datasets->auto-gptq) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from aiohttp->datasets->auto-gptq) (1.18.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from requests->transformers>=4.31.0->auto-gptq) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from requests->transformers>=4.31.0->auto-gptq) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from requests->transformers>=4.31.0->auto-gptq) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from requests->transformers>=4.31.0->auto-gptq) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->auto-gptq) (2.1.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from pandas->datasets->auto-gptq) (2.9.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from pandas->datasets->auto-gptq) (2024.2)\n",
            "Downloading auto_gptq-0.7.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (23.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.5/23.5 MB\u001b[0m \u001b[31m46.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
            "\u001b[?25hDownloading peft-0.14.0-py3-none-any.whl (374 kB)\n",
            "Downloading gekko-1.2.1-py3-none-any.whl (13.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.2/13.2 MB\u001b[0m \u001b[31m58.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
            "\u001b[?25hDownloading rouge-1.0.1-py3-none-any.whl (13 kB)\n",
            "Downloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m49.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sentencepiece, rouge, gekko, peft, auto-gptq\n",
            "Successfully installed auto-gptq-0.7.1 gekko-1.2.1 peft-0.14.0 rouge-1.0.1 sentencepiece-0.2.0\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "# the crux package for the project\n",
        "%pip install auto-gptq --extra-index-url https://huggingface.github.io/autogptq-index/whl/cu117/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Name: auto_gptq\n",
            "Version: 0.7.1\n",
            "Summary: An easy-to-use LLMs quantization package with user-friendly apis, based on GPTQ algorithm.\n",
            "Home-page: https://github.com/PanQiWei/AutoGPTQ\n",
            "Author: PanQiWei\n",
            "Author-email: \n",
            "License: \n",
            "Location: /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages\n",
            "Requires: accelerate, datasets, gekko, numpy, peft, rouge, safetensors, sentencepiece, torch, tqdm, transformers\n",
            "Required-by: \n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip show auto-gptq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found existing installation: torch 2.5.1\n",
            "Uninstalling torch-2.5.1:\n",
            "  Successfully uninstalled torch-2.5.1\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "# install the torch library for using torch version with CUDA\n",
        "#%pip uninstall torch -y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu117\n",
            "Requirement already satisfied: torch in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (2.5.1)\n",
            "Requirement already satisfied: filelock in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch) (2023.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from jinja2->torch) (2.1.5)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install torch --index-url https://download.pytorch.org/whl/cu117"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Name: torch\n",
            "Version: 2.5.1\n",
            "Summary: Tensors and Dynamic neural networks in Python with strong GPU acceleration\n",
            "Home-page: https://pytorch.org/\n",
            "Author: PyTorch Team\n",
            "Author-email: packages@pytorch.org\n",
            "License: BSD-3-Clause\n",
            "Location: /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages\n",
            "Requires: filelock, fsspec, jinja2, networkx, nvidia-cublas-cu12, nvidia-cuda-cupti-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-runtime-cu12, nvidia-cudnn-cu12, nvidia-cufft-cu12, nvidia-curand-cu12, nvidia-cusolver-cu12, nvidia-cusparse-cu12, nvidia-nccl-cu12, nvidia-nvjitlink-cu12, nvidia-nvtx-cu12, sympy, triton, typing-extensions\n",
            "Required-by: accelerate, auto_gptq, bitsandbytes, optimum, peft\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip show torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, GPTQConfig\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Is CUDA available: True\n",
            "CUDA device: Tesla T4\n"
          ]
        }
      ],
      "source": [
        "print(f\"Is CUDA available: {torch.cuda.is_available()}\")\n",
        "print(f\"CUDA device: {torch.cuda.get_device_name(torch.cuda.current_device())}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:411: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "  def forward(ctx, input, qweight, scales, qzeros, g_idx, bits, maxq):\n",
            "/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:419: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
            "  def backward(ctx, grad_output):\n",
            "/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:461: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "  @custom_fwd(cast_inputs=torch.float16)\n",
            "CUDA extension not installed.\n",
            "CUDA extension not installed.\n"
          ]
        }
      ],
      "source": [
        "import optimum\n",
        "import auto_gptq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True\n",
            "True\n"
          ]
        }
      ],
      "source": [
        "from transformers.utils import is_auto_gptq_available, is_optimum_available\n",
        "print(is_auto_gptq_available())\n",
        "print(is_optimum_available())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda:0\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using `disable_exllama` is deprecated and will be removed in version 4.37. Use `use_exllama` instead and specify the version with `exllama_config`.The value of `use_exllama` will be overwritten by `disable_exllama` passed in `GPTQConfig` or stored in your config file.\n",
            "Using `disable_exllama` is deprecated and will be removed in version 4.37. Use `use_exllama` instead and specify the version with `exllama_config`.The value of `use_exllama` will be overwritten by `disable_exllama` passed in `GPTQConfig` or stored in your config file.\n",
            "/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/transformers/quantizers/auto.py:195: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.However, loading attributes (e.g. ['use_cuda_fp16', 'use_exllama', 'max_input_length', 'exllama_config', 'disable_exllama']) will be overwritten with the one you passed to `from_pretrained`. The rest will be ignored.\n",
            "  warnings.warn(warning_msg)\n",
            "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n",
            "Some weights of the model checkpoint at TheBloke/zephyr-7B-beta-GPTQ were not used when initializing MistralForCausalLM: {'model.layers.0.self_attn.v_proj.bias', 'model.layers.18.mlp.up_proj.bias', 'model.layers.14.mlp.gate_proj.bias', 'model.layers.1.self_attn.v_proj.bias', 'model.layers.20.self_attn.q_proj.bias', 'model.layers.28.mlp.down_proj.bias', 'model.layers.31.mlp.up_proj.bias', 'model.layers.4.self_attn.k_proj.bias', 'model.layers.13.self_attn.o_proj.bias', 'model.layers.18.mlp.down_proj.bias', 'model.layers.24.self_attn.o_proj.bias', 'model.layers.7.mlp.down_proj.bias', 'model.layers.4.self_attn.v_proj.bias', 'model.layers.7.mlp.up_proj.bias', 'model.layers.19.mlp.down_proj.bias', 'model.layers.19.self_attn.o_proj.bias', 'model.layers.14.mlp.down_proj.bias', 'model.layers.1.mlp.down_proj.bias', 'model.layers.23.self_attn.v_proj.bias', 'model.layers.7.mlp.gate_proj.bias', 'model.layers.27.mlp.up_proj.bias', 'model.layers.9.mlp.up_proj.bias', 'model.layers.5.mlp.up_proj.bias', 'model.layers.7.self_attn.o_proj.bias', 'model.layers.6.mlp.gate_proj.bias', 'model.layers.2.mlp.up_proj.bias', 'model.layers.27.mlp.down_proj.bias', 'model.layers.21.mlp.gate_proj.bias', 'model.layers.4.mlp.down_proj.bias', 'model.layers.0.self_attn.k_proj.bias', 'model.layers.26.mlp.up_proj.bias', 'model.layers.29.mlp.up_proj.bias', 'model.layers.23.self_attn.o_proj.bias', 'model.layers.12.mlp.up_proj.bias', 'model.layers.17.self_attn.v_proj.bias', 'model.layers.2.self_attn.q_proj.bias', 'model.layers.22.self_attn.v_proj.bias', 'model.layers.19.mlp.up_proj.bias', 'model.layers.7.self_attn.k_proj.bias', 'model.layers.16.self_attn.o_proj.bias', 'model.layers.8.self_attn.o_proj.bias', 'model.layers.11.self_attn.k_proj.bias', 'model.layers.24.self_attn.v_proj.bias', 'model.layers.13.self_attn.q_proj.bias', 'model.layers.19.self_attn.v_proj.bias', 'model.layers.30.self_attn.o_proj.bias', 'model.layers.30.self_attn.q_proj.bias', 'model.layers.11.mlp.up_proj.bias', 'model.layers.20.mlp.down_proj.bias', 'model.layers.10.self_attn.o_proj.bias', 'model.layers.28.self_attn.q_proj.bias', 'model.layers.3.mlp.gate_proj.bias', 'model.layers.14.self_attn.v_proj.bias', 'model.layers.23.mlp.down_proj.bias', 'model.layers.20.self_attn.k_proj.bias', 'model.layers.22.mlp.up_proj.bias', 'model.layers.22.self_attn.o_proj.bias', 'model.layers.22.mlp.down_proj.bias', 'model.layers.21.self_attn.v_proj.bias', 'model.layers.27.self_attn.k_proj.bias', 'model.layers.6.self_attn.k_proj.bias', 'model.layers.27.self_attn.v_proj.bias', 'model.layers.10.self_attn.v_proj.bias', 'model.layers.16.self_attn.v_proj.bias', 'model.layers.20.self_attn.o_proj.bias', 'model.layers.9.self_attn.o_proj.bias', 'model.layers.27.self_attn.o_proj.bias', 'model.layers.15.self_attn.v_proj.bias', 'model.layers.29.self_attn.v_proj.bias', 'model.layers.11.mlp.down_proj.bias', 'model.layers.26.self_attn.k_proj.bias', 'model.layers.9.mlp.down_proj.bias', 'model.layers.25.self_attn.q_proj.bias', 'model.layers.1.self_attn.k_proj.bias', 'model.layers.28.self_attn.o_proj.bias', 'model.layers.14.self_attn.k_proj.bias', 'model.layers.27.self_attn.q_proj.bias', 'model.layers.23.mlp.up_proj.bias', 'model.layers.31.self_attn.o_proj.bias', 'model.layers.0.mlp.up_proj.bias', 'model.layers.26.self_attn.q_proj.bias', 'model.layers.26.self_attn.o_proj.bias', 'model.layers.28.mlp.up_proj.bias', 'model.layers.14.self_attn.q_proj.bias', 'model.layers.5.self_attn.k_proj.bias', 'model.layers.31.self_attn.k_proj.bias', 'model.layers.24.mlp.down_proj.bias', 'model.layers.21.self_attn.k_proj.bias', 'model.layers.7.self_attn.v_proj.bias', 'model.layers.26.mlp.down_proj.bias', 'model.layers.0.self_attn.o_proj.bias', 'model.layers.23.self_attn.k_proj.bias', 'model.layers.24.mlp.up_proj.bias', 'model.layers.2.self_attn.o_proj.bias', 'model.layers.15.self_attn.o_proj.bias', 'model.layers.7.self_attn.q_proj.bias', 'model.layers.12.mlp.down_proj.bias', 'model.layers.10.self_attn.q_proj.bias', 'model.layers.16.mlp.up_proj.bias', 'model.layers.16.self_attn.q_proj.bias', 'model.layers.8.mlp.up_proj.bias', 'model.layers.31.mlp.down_proj.bias', 'model.layers.22.self_attn.q_proj.bias', 'model.layers.23.self_attn.q_proj.bias', 'model.layers.11.self_attn.o_proj.bias', 'model.layers.3.mlp.down_proj.bias', 'model.layers.24.mlp.gate_proj.bias', 'model.layers.0.mlp.down_proj.bias', 'model.layers.15.mlp.down_proj.bias', 'model.layers.16.mlp.gate_proj.bias', 'model.layers.20.self_attn.v_proj.bias', 'model.layers.17.self_attn.q_proj.bias', 'model.layers.21.mlp.down_proj.bias', 'model.layers.25.mlp.gate_proj.bias', 'model.layers.30.mlp.up_proj.bias', 'model.layers.6.self_attn.q_proj.bias', 'model.layers.10.mlp.up_proj.bias', 'model.layers.11.mlp.gate_proj.bias', 'model.layers.12.self_attn.v_proj.bias', 'model.layers.25.self_attn.k_proj.bias', 'model.layers.9.mlp.gate_proj.bias', 'model.layers.17.self_attn.k_proj.bias', 'model.layers.29.self_attn.q_proj.bias', 'model.layers.21.mlp.up_proj.bias', 'model.layers.3.mlp.up_proj.bias', 'model.layers.5.mlp.gate_proj.bias', 'model.layers.15.self_attn.k_proj.bias', 'model.layers.18.self_attn.q_proj.bias', 'model.layers.17.mlp.gate_proj.bias', 'model.layers.10.self_attn.k_proj.bias', 'model.layers.25.self_attn.o_proj.bias', 'model.layers.28.self_attn.v_proj.bias', 'model.layers.27.mlp.gate_proj.bias', 'model.layers.17.self_attn.o_proj.bias', 'model.layers.28.mlp.gate_proj.bias', 'model.layers.10.mlp.down_proj.bias', 'model.layers.31.self_attn.q_proj.bias', 'model.layers.13.mlp.up_proj.bias', 'model.layers.30.mlp.gate_proj.bias', 'model.layers.6.self_attn.o_proj.bias', 'model.layers.5.self_attn.o_proj.bias', 'model.layers.13.self_attn.k_proj.bias', 'model.layers.14.self_attn.o_proj.bias', 'model.layers.25.self_attn.v_proj.bias', 'model.layers.12.self_attn.k_proj.bias', 'model.layers.1.mlp.up_proj.bias', 'model.layers.8.self_attn.k_proj.bias', 'model.layers.21.self_attn.o_proj.bias', 'model.layers.13.self_attn.v_proj.bias', 'model.layers.10.mlp.gate_proj.bias', 'model.layers.9.self_attn.k_proj.bias', 'model.layers.31.mlp.gate_proj.bias', 'model.layers.8.self_attn.v_proj.bias', 'model.layers.4.mlp.up_proj.bias', 'model.layers.18.mlp.gate_proj.bias', 'model.layers.15.mlp.up_proj.bias', 'model.layers.29.self_attn.k_proj.bias', 'model.layers.8.mlp.gate_proj.bias', 'model.layers.18.self_attn.k_proj.bias', 'model.layers.3.self_attn.q_proj.bias', 'model.layers.19.self_attn.q_proj.bias', 'model.layers.19.self_attn.k_proj.bias', 'model.layers.25.mlp.down_proj.bias', 'model.layers.9.self_attn.q_proj.bias', 'model.layers.2.self_attn.k_proj.bias', 'model.layers.22.mlp.gate_proj.bias', 'model.layers.20.mlp.gate_proj.bias', 'model.layers.17.mlp.down_proj.bias', 'model.layers.2.self_attn.v_proj.bias', 'model.layers.18.self_attn.o_proj.bias', 'model.layers.29.self_attn.o_proj.bias', 'model.layers.4.self_attn.q_proj.bias', 'model.layers.0.self_attn.q_proj.bias', 'model.layers.28.self_attn.k_proj.bias', 'model.layers.3.self_attn.v_proj.bias', 'model.layers.12.self_attn.o_proj.bias', 'model.layers.13.mlp.gate_proj.bias', 'model.layers.2.mlp.down_proj.bias', 'model.layers.19.mlp.gate_proj.bias', 'model.layers.24.self_attn.q_proj.bias', 'model.layers.18.self_attn.v_proj.bias', 'model.layers.11.self_attn.q_proj.bias', 'model.layers.26.self_attn.v_proj.bias', 'model.layers.6.mlp.down_proj.bias', 'model.layers.30.self_attn.v_proj.bias', 'model.layers.1.self_attn.o_proj.bias', 'model.layers.15.mlp.gate_proj.bias', 'model.layers.9.self_attn.v_proj.bias', 'model.layers.6.mlp.up_proj.bias', 'model.layers.1.mlp.gate_proj.bias', 'model.layers.21.self_attn.q_proj.bias', 'model.layers.30.mlp.down_proj.bias', 'model.layers.5.self_attn.v_proj.bias', 'model.layers.24.self_attn.k_proj.bias', 'model.layers.26.mlp.gate_proj.bias', 'model.layers.4.mlp.gate_proj.bias', 'model.layers.12.mlp.gate_proj.bias', 'model.layers.5.self_attn.q_proj.bias', 'model.layers.2.mlp.gate_proj.bias', 'model.layers.13.mlp.down_proj.bias', 'model.layers.8.mlp.down_proj.bias', 'model.layers.14.mlp.up_proj.bias', 'model.layers.23.mlp.gate_proj.bias', 'model.layers.29.mlp.down_proj.bias', 'model.layers.0.mlp.gate_proj.bias', 'model.layers.16.self_attn.k_proj.bias', 'model.layers.15.self_attn.q_proj.bias', 'model.layers.3.self_attn.k_proj.bias', 'model.layers.30.self_attn.k_proj.bias', 'model.layers.1.self_attn.q_proj.bias', 'model.layers.12.self_attn.q_proj.bias', 'model.layers.29.mlp.gate_proj.bias', 'model.layers.22.self_attn.k_proj.bias', 'model.layers.4.self_attn.o_proj.bias', 'model.layers.31.self_attn.v_proj.bias', 'model.layers.3.self_attn.o_proj.bias', 'model.layers.17.mlp.up_proj.bias', 'model.layers.5.mlp.down_proj.bias', 'model.layers.6.self_attn.v_proj.bias', 'model.layers.20.mlp.up_proj.bias', 'model.layers.16.mlp.down_proj.bias', 'model.layers.8.self_attn.q_proj.bias', 'model.layers.11.self_attn.v_proj.bias', 'model.layers.25.mlp.up_proj.bias'}\n",
            "- This IS expected if you are initializing MistralForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing MistralForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MistralForCausalLM(\n",
            "  (model): MistralModel(\n",
            "    (embed_tokens): Embedding(32000, 4096, padding_idx=2)\n",
            "    (layers): ModuleList(\n",
            "      (0-31): 32 x MistralDecoderLayer(\n",
            "        (self_attn): MistralAttention(\n",
            "          (k_proj): QuantLinear()\n",
            "          (o_proj): QuantLinear()\n",
            "          (q_proj): QuantLinear()\n",
            "          (v_proj): QuantLinear()\n",
            "        )\n",
            "        (mlp): MistralMLP(\n",
            "          (act_fn): SiLU()\n",
            "          (down_proj): QuantLinear()\n",
            "          (gate_proj): QuantLinear()\n",
            "          (up_proj): QuantLinear()\n",
            "        )\n",
            "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
            "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
            "      )\n",
            "    )\n",
            "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
            "    (rotary_emb): MistralRotaryEmbedding()\n",
            "  )\n",
            "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "# The model name is the name of the model you want to use, you can use the model from HuggingFace or the model you trained\n",
        "# The model is a pretrained model from the Hugging space created by TheBloke (cf https://huggingface.co/TheBloke)\n",
        "# Further infos for GPTQ model availbale here : https://huggingface.co/docs/transformers/quantization/gptq\n",
        "\n",
        "model_name = \"TheBloke/zephyr-7B-beta-GPTQ\" \n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name,use_fast=True,padding_side=\"left\")\n",
        "#if training disable_exllama=True\n",
        "quantization_config_loading = GPTQConfig(\n",
        "                                bits=4,\n",
        "                                group_size=128,\n",
        "                                disable_exllama=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=quantization_config_loading, device_map=\"auto\")\n",
        "model = model.to(device)\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'quant_method': <QuantizationMethod.GPTQ: 'gptq'>,\n",
              " 'bits': 4,\n",
              " 'tokenizer': None,\n",
              " 'dataset': None,\n",
              " 'group_size': 128,\n",
              " 'damp_percent': 0.1,\n",
              " 'desc_act': True,\n",
              " 'sym': True,\n",
              " 'true_sequential': True,\n",
              " 'use_cuda_fp16': False,\n",
              " 'model_seqlen': 4096,\n",
              " 'block_name_to_quantize': 'model.layers',\n",
              " 'module_name_preceding_first_block': ['model.embed_tokens'],\n",
              " 'batch_size': 1,\n",
              " 'pad_token_id': None,\n",
              " 'use_exllama': False,\n",
              " 'max_input_length': None,\n",
              " 'exllama_config': {'version': <ExllamaVersion.ONE: 1>},\n",
              " 'cache_block_outputs': True,\n",
              " 'modules_in_block_to_quantize': None}"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.config.quantization_config.to_dict()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Testing the pretrained model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_text(input_text,max_new_tokens=512,top_k=50,top_p=0.95,temperature=0.7,no_grad=False):\n",
        "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "    input_ids = tokenizer.encode(input_text, padding=True, return_tensors=\"pt\").to(device)\n",
        "    attention_mask = input_ids.ne(tokenizer.pad_token_id).long().to(device)\n",
        "    output = None\n",
        "    if no_grad:\n",
        "        with torch.no_grad():\n",
        "            output = model.generate(input_ids, attention_mask=attention_mask, max_new_tokens=max_new_tokens, top_k=top_k, top_p=top_p, temperature=temperature,do_sample=True)\n",
        "    else:\n",
        "        output = model.generate(input_ids, attention_mask=attention_mask, max_new_tokens=max_new_tokens, top_k=top_k, top_p=top_p, temperature=temperature,do_sample=True)\n",
        "    return tokenizer.decode(output[0], skip_special_tokens=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "#so test by generating a story with the following prompt\n",
        "prompt = \"Once upon a time an ice-cream met a spoon and they fell in love\"\n",
        "prompt2 = \"Hello my name is\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prompt : Once upon a time an ice-cream met a spoon and they fell in love \n",
            "\n",
            "Story : Once upon a time an ice-cream met a spoon and they fell in love... This is their fairytale-like love story, set in a magical garden full of lollies and sprinkles and ice-creams, and told with the help of beautiful illustrations. And in the end, it will make you hungry for ice-cream!\n",
            "This is the second book by Tania CC in the Tales series. Like all the Tales stories, it is beautifully illustrated by Ana Kovačić and is a fairytale in the true \n",
            "\n",
            "Time to generate the story : 97.36324572563171 s\n"
          ]
        }
      ],
      "source": [
        "#overview of the story\n",
        "import time\n",
        "\n",
        "print(f'Prompt : {prompt} \\n')\n",
        "start = time.time()\n",
        "print(f'Story : {generate_text(prompt,max_new_tokens=100,temperature=1)} \\n')\n",
        "end = time.time()\n",
        "print(f'Time to generate the story : {end-start} s')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hello my name is Katie and I am a 20 year old girl from the UK. I have been a member of this site for a few months now and I have been reading a lot of the posts and I have noticed that there are a lot of people who are struggling with their mental health and I wanted to share my story with you all.\n",
            "\n",
            "I have suffered from anxiety and depression for as long as I can remember. I remember being a child and feeling like I didn't fit in with\n"
          ]
        }
      ],
      "source": [
        "print(f'{generate_text(prompt2,max_new_tokens=100,temperature=0.95)} \\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## If you want to see the use of the current hardware (GPU, CPU)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: psutil in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (6.0.0)\n",
            "Collecting gputil\n",
            "  Downloading GPUtil-1.4.0.tar.gz (5.5 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25hBuilding wheels for collected packages: gputil\n",
            "  Building wheel for gputil (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for gputil: filename=GPUtil-1.4.0-py3-none-any.whl size=7392 sha256=a58fd3059df40b0e4982a455fac23df7c4e6764b950c4c5d1d72f1095e4c7ece\n",
            "  Stored in directory: /home/azureuser/.cache/pip/wheels/a9/8a/bd/81082387151853ab8b6b3ef33426e98f5cbfebc3c397a9d4d0\n",
            "Successfully built gputil\n",
            "Installing collected packages: gputil\n",
            "Successfully installed gputil-1.4.0\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "CPU Utilization: 0.0%\n",
            "Memory Utilization: 21.1%\n",
            "  ID  Name      Load    Free Memory    Used Memory    Total Memory    Temperature\n",
            "----  --------  ------  -------------  -------------  --------------  -------------\n",
            "   0  Tesla T4  0.0%    11055.0MB      4893.0MB       16384.0MB       36.0 C\n",
            "CPU Utilization: 0.0%\n",
            "Memory Utilization: 21.1%\n",
            "  ID  Name      Load    Free Memory    Used Memory    Total Memory    Temperature\n",
            "----  --------  ------  -------------  -------------  --------------  -------------\n",
            "   0  Tesla T4  0.0%    11055.0MB      4893.0MB       16384.0MB       36.0 C\n",
            "CPU Utilization: 2.4%\n",
            "Memory Utilization: 21.2%\n",
            "  ID  Name      Load    Free Memory    Used Memory    Total Memory    Temperature\n",
            "----  --------  ------  -------------  -------------  --------------  -------------\n",
            "   0  Tesla T4  0.0%    11055.0MB      4893.0MB       16384.0MB       36.0 C\n",
            "CPU Utilization: 0.0%\n",
            "Memory Utilization: 21.1%\n",
            "  ID  Name      Load    Free Memory    Used Memory    Total Memory    Temperature\n",
            "----  --------  ------  -------------  -------------  --------------  -------------\n",
            "   0  Tesla T4  0.0%    11055.0MB      4893.0MB       16384.0MB       36.0 C\n",
            "CPU Utilization: 0.0%\n",
            "Memory Utilization: 21.1%\n",
            "  ID  Name      Load    Free Memory    Used Memory    Total Memory    Temperature\n",
            "----  --------  ------  -------------  -------------  --------------  -------------\n",
            "   0  Tesla T4  0.0%    11055.0MB      4893.0MB       16384.0MB       36.0 C\n",
            "CPU Utilization: 0.0%\n",
            "Memory Utilization: 21.1%\n",
            "  ID  Name      Load    Free Memory    Used Memory    Total Memory    Temperature\n",
            "----  --------  ------  -------------  -------------  --------------  -------------\n",
            "   0  Tesla T4  0.0%    11055.0MB      4893.0MB       16384.0MB       36.0 C\n",
            "CPU Utilization: 0.0%\n",
            "Memory Utilization: 21.1%\n",
            "  ID  Name      Load    Free Memory    Used Memory    Total Memory    Temperature\n",
            "----  --------  ------  -------------  -------------  --------------  -------------\n",
            "   0  Tesla T4  0.0%    11055.0MB      4893.0MB       16384.0MB       36.0 C\n",
            "CPU Utilization: 2.5%\n",
            "Memory Utilization: 21.1%\n",
            "  ID  Name      Load    Free Memory    Used Memory    Total Memory    Temperature\n",
            "----  --------  ------  -------------  -------------  --------------  -------------\n",
            "   0  Tesla T4  0.0%    11055.0MB      4893.0MB       16384.0MB       36.0 C\n",
            "CPU Utilization: 0.0%\n",
            "Memory Utilization: 21.2%\n",
            "  ID  Name      Load    Free Memory    Used Memory    Total Memory    Temperature\n",
            "----  --------  ------  -------------  -------------  --------------  -------------\n",
            "   0  Tesla T4  0.0%    11055.0MB      4893.0MB       16384.0MB       36.0 C\n",
            "CPU Utilization: 0.0%\n",
            "Memory Utilization: 21.2%\n",
            "  ID  Name      Load    Free Memory    Used Memory    Total Memory    Temperature\n",
            "----  --------  ------  -------------  -------------  --------------  -------------\n",
            "   0  Tesla T4  0.0%    11055.0MB      4893.0MB       16384.0MB       36.0 C\n",
            "CPU Utilization: 0.0%\n",
            "Memory Utilization: 21.2%\n",
            "  ID  Name      Load    Free Memory    Used Memory    Total Memory    Temperature\n",
            "----  --------  ------  -------------  -------------  --------------  -------------\n",
            "   0  Tesla T4  0.0%    11055.0MB      4893.0MB       16384.0MB       36.0 C\n",
            "CPU Utilization: 2.5%\n",
            "Memory Utilization: 21.1%\n",
            "  ID  Name      Load    Free Memory    Used Memory    Total Memory    Temperature\n",
            "----  --------  ------  -------------  -------------  --------------  -------------\n",
            "   0  Tesla T4  0.0%    11055.0MB      4893.0MB       16384.0MB       36.0 C\n",
            "CPU Utilization: 0.0%\n",
            "Memory Utilization: 21.1%\n",
            "  ID  Name      Load    Free Memory    Used Memory    Total Memory    Temperature\n",
            "----  --------  ------  -------------  -------------  --------------  -------------\n",
            "   0  Tesla T4  0.0%    11055.0MB      4893.0MB       16384.0MB       36.0 C\n",
            "CPU Utilization: 0.0%\n",
            "Memory Utilization: 21.1%\n",
            "  ID  Name      Load    Free Memory    Used Memory    Total Memory    Temperature\n",
            "----  --------  ------  -------------  -------------  --------------  -------------\n",
            "   0  Tesla T4  0.0%    11055.0MB      4893.0MB       16384.0MB       36.0 C\n",
            "CPU Utilization: 0.0%\n",
            "Memory Utilization: 21.1%\n",
            "  ID  Name      Load    Free Memory    Used Memory    Total Memory    Temperature\n",
            "----  --------  ------  -------------  -------------  --------------  -------------\n",
            "   0  Tesla T4  0.0%    11055.0MB      4893.0MB       16384.0MB       36.0 C\n",
            "CPU Utilization: 0.0%\n",
            "Memory Utilization: 21.2%\n",
            "  ID  Name      Load    Free Memory    Used Memory    Total Memory    Temperature\n",
            "----  --------  ------  -------------  -------------  --------------  -------------\n",
            "   0  Tesla T4  0.0%    11055.0MB      4893.0MB       16384.0MB       36.0 C\n",
            "CPU Utilization: 0.0%\n",
            "Memory Utilization: 21.2%\n",
            "  ID  Name      Load    Free Memory    Used Memory    Total Memory    Temperature\n",
            "----  --------  ------  -------------  -------------  --------------  -------------\n",
            "   0  Tesla T4  0.0%    11055.0MB      4893.0MB       16384.0MB       36.0 C\n",
            "CPU Utilization: 2.5%\n",
            "Memory Utilization: 21.2%\n",
            "  ID  Name      Load    Free Memory    Used Memory    Total Memory    Temperature\n",
            "----  --------  ------  -------------  -------------  --------------  -------------\n",
            "   0  Tesla T4  0.0%    11055.0MB      4893.0MB       16384.0MB       36.0 C\n",
            "CPU Utilization: 0.0%\n",
            "Memory Utilization: 21.2%\n",
            "  ID  Name      Load    Free Memory    Used Memory    Total Memory    Temperature\n",
            "----  --------  ------  -------------  -------------  --------------  -------------\n",
            "   0  Tesla T4  0.0%    11055.0MB      4893.0MB       16384.0MB       36.0 C\n",
            "CPU Utilization: 0.0%\n",
            "Memory Utilization: 21.1%\n",
            "  ID  Name      Load    Free Memory    Used Memory    Total Memory    Temperature\n",
            "----  --------  ------  -------------  -------------  --------------  -------------\n",
            "   0  Tesla T4  0.0%    11055.0MB      4893.0MB       16384.0MB       36.0 C\n",
            "CPU Utilization: 0.0%\n",
            "Memory Utilization: 21.1%\n",
            "  ID  Name      Load    Free Memory    Used Memory    Total Memory    Temperature\n",
            "----  --------  ------  -------------  -------------  --------------  -------------\n",
            "   0  Tesla T4  0.0%    11055.0MB      4893.0MB       16384.0MB       36.0 C\n",
            "CPU Utilization: 0.0%\n",
            "Memory Utilization: 21.1%\n",
            "  ID  Name      Load    Free Memory    Used Memory    Total Memory    Temperature\n",
            "----  --------  ------  -------------  -------------  --------------  -------------\n",
            "   0  Tesla T4  0.0%    11055.0MB      4893.0MB       16384.0MB       36.0 C\n",
            "CPU Utilization: 0.0%\n",
            "Memory Utilization: 21.1%\n",
            "  ID  Name      Load    Free Memory    Used Memory    Total Memory    Temperature\n",
            "----  --------  ------  -------------  -------------  --------------  -------------\n",
            "   0  Tesla T4  0.0%    11055.0MB      4893.0MB       16384.0MB       36.0 C\n",
            "CPU Utilization: 0.0%\n",
            "Memory Utilization: 21.1%\n",
            "  ID  Name      Load    Free Memory    Used Memory    Total Memory    Temperature\n",
            "----  --------  ------  -------------  -------------  --------------  -------------\n",
            "   0  Tesla T4  0.0%    11055.0MB      4893.0MB       16384.0MB       36.0 C\n",
            "CPU Utilization: 2.5%\n",
            "Memory Utilization: 21.1%\n",
            "  ID  Name      Load    Free Memory    Used Memory    Total Memory    Temperature\n",
            "----  --------  ------  -------------  -------------  --------------  -------------\n",
            "   0  Tesla T4  0.0%    11055.0MB      4893.0MB       16384.0MB       36.0 C\n",
            "CPU Utilization: 0.0%\n",
            "Memory Utilization: 21.1%\n",
            "  ID  Name      Load    Free Memory    Used Memory    Total Memory    Temperature\n",
            "----  --------  ------  -------------  -------------  --------------  -------------\n",
            "   0  Tesla T4  0.0%    11055.0MB      4893.0MB       16384.0MB       36.0 C\n",
            "CPU Utilization: 23.7%\n",
            "Memory Utilization: 21.2%\n",
            "  ID  Name      Load    Free Memory    Used Memory    Total Memory    Temperature\n",
            "----  --------  ------  -------------  -------------  --------------  -------------\n",
            "   0  Tesla T4  0.0%    11055.0MB      4893.0MB       16384.0MB       36.0 C\n",
            "CPU Utilization: 0.0%\n",
            "Memory Utilization: 21.2%\n",
            "  ID  Name      Load    Free Memory    Used Memory    Total Memory    Temperature\n",
            "----  --------  ------  -------------  -------------  --------------  -------------\n",
            "   0  Tesla T4  0.0%    11055.0MB      4893.0MB       16384.0MB       36.0 C\n",
            "CPU Utilization: 0.0%\n",
            "Memory Utilization: 21.2%\n",
            "  ID  Name      Load    Free Memory    Used Memory    Total Memory    Temperature\n",
            "----  --------  ------  -------------  -------------  --------------  -------------\n",
            "   0  Tesla T4  0.0%    11055.0MB      4893.0MB       16384.0MB       36.0 C\n",
            "CPU Utilization: 0.0%\n",
            "Memory Utilization: 21.3%\n",
            "  ID  Name      Load    Free Memory    Used Memory    Total Memory    Temperature\n",
            "----  --------  ------  -------------  -------------  --------------  -------------\n",
            "   0  Tesla T4  0.0%    11055.0MB      4893.0MB       16384.0MB       36.0 C\n",
            "CPU Utilization: 97.4%\n",
            "Memory Utilization: 20.8%\n",
            "  ID  Name      Load    Free Memory    Used Memory    Total Memory    Temperature\n",
            "----  --------  ------  -------------  -------------  --------------  -------------\n",
            "   0  Tesla T4  0.0%    11055.0MB      4893.0MB       16384.0MB       36.0 C\n",
            "CPU Utilization: 0.0%\n",
            "Memory Utilization: 20.8%\n",
            "  ID  Name      Load    Free Memory    Used Memory    Total Memory    Temperature\n",
            "----  --------  ------  -------------  -------------  --------------  -------------\n",
            "   0  Tesla T4  0.0%    11055.0MB      4893.0MB       16384.0MB       36.0 C\n",
            "CPU Utilization: 2.4%\n",
            "Memory Utilization: 20.9%\n",
            "  ID  Name      Load    Free Memory    Used Memory    Total Memory    Temperature\n",
            "----  --------  ------  -------------  -------------  --------------  -------------\n",
            "   0  Tesla T4  0.0%    11055.0MB      4893.0MB       16384.0MB       36.0 C\n",
            "CPU Utilization: 4.9%\n",
            "Memory Utilization: 20.9%\n",
            "  ID  Name      Load    Free Memory    Used Memory    Total Memory    Temperature\n",
            "----  --------  ------  -------------  -------------  --------------  -------------\n",
            "   0  Tesla T4  0.0%    11055.0MB      4893.0MB       16384.0MB       36.0 C\n",
            "CPU Utilization: 0.0%\n",
            "Memory Utilization: 20.8%\n",
            "  ID  Name      Load    Free Memory    Used Memory    Total Memory    Temperature\n",
            "----  --------  ------  -------------  -------------  --------------  -------------\n",
            "   0  Tesla T4  0.0%    11055.0MB      4893.0MB       16384.0MB       36.0 C\n",
            "CPU Utilization: 0.0%\n",
            "Memory Utilization: 20.8%\n",
            "  ID  Name      Load    Free Memory    Used Memory    Total Memory    Temperature\n",
            "----  --------  ------  -------------  -------------  --------------  -------------\n",
            "   0  Tesla T4  0.0%    11055.0MB      4893.0MB       16384.0MB       36.0 C\n",
            "CPU Utilization: 2.4%\n",
            "Memory Utilization: 20.8%\n",
            "  ID  Name      Load    Free Memory    Used Memory    Total Memory    Temperature\n",
            "----  --------  ------  -------------  -------------  --------------  -------------\n",
            "   0  Tesla T4  0.0%    11055.0MB      4893.0MB       16384.0MB       36.0 C\n",
            "CPU Utilization: 0.0%\n",
            "Memory Utilization: 20.8%\n",
            "  ID  Name      Load    Free Memory    Used Memory    Total Memory    Temperature\n",
            "----  --------  ------  -------------  -------------  --------------  -------------\n",
            "   0  Tesla T4  0.0%    11055.0MB      4893.0MB       16384.0MB       36.0 C\n",
            "CPU Utilization: 2.5%\n",
            "Memory Utilization: 20.8%\n",
            "  ID  Name      Load    Free Memory    Used Memory    Total Memory    Temperature\n",
            "----  --------  ------  -------------  -------------  --------------  -------------\n",
            "   0  Tesla T4  0.0%    11055.0MB      4893.0MB       16384.0MB       36.0 C\n",
            "CPU Utilization: 0.0%\n",
            "Memory Utilization: 20.8%\n",
            "  ID  Name      Load    Free Memory    Used Memory    Total Memory    Temperature\n",
            "----  --------  ------  -------------  -------------  --------------  -------------\n",
            "   0  Tesla T4  0.0%    11055.0MB      4893.0MB       16384.0MB       36.0 C\n",
            "CPU Utilization: 0.0%\n",
            "Memory Utilization: 20.8%\n",
            "  ID  Name      Load    Free Memory    Used Memory    Total Memory    Temperature\n",
            "----  --------  ------  -------------  -------------  --------------  -------------\n",
            "   0  Tesla T4  0.0%    11055.0MB      4893.0MB       16384.0MB       36.0 C\n",
            "CPU Utilization: 0.0%\n",
            "Memory Utilization: 20.8%\n",
            "  ID  Name      Load    Free Memory    Used Memory    Total Memory    Temperature\n",
            "----  --------  ------  -------------  -------------  --------------  -------------\n",
            "   0  Tesla T4  0.0%    11055.0MB      4893.0MB       16384.0MB       36.0 C\n",
            "CPU Utilization: 0.0%\n",
            "Memory Utilization: 20.8%\n",
            "  ID  Name      Load    Free Memory    Used Memory    Total Memory    Temperature\n",
            "----  --------  ------  -------------  -------------  --------------  -------------\n",
            "   0  Tesla T4  0.0%    11055.0MB      4893.0MB       16384.0MB       36.0 C\n",
            "CPU Utilization: 0.0%\n",
            "Memory Utilization: 20.8%\n",
            "  ID  Name      Load    Free Memory    Used Memory    Total Memory    Temperature\n",
            "----  --------  ------  -------------  -------------  --------------  -------------\n",
            "   0  Tesla T4  0.0%    11055.0MB      4893.0MB       16384.0MB       36.0 C\n",
            "CPU Utilization: 0.0%\n",
            "Memory Utilization: 20.8%\n",
            "  ID  Name      Load    Free Memory    Used Memory    Total Memory    Temperature\n",
            "----  --------  ------  -------------  -------------  --------------  -------------\n",
            "   0  Tesla T4  0.0%    11055.0MB      4893.0MB       16384.0MB       36.0 C\n",
            "CPU Utilization: 0.0%\n",
            "Memory Utilization: 20.8%\n",
            "  ID  Name      Load    Free Memory    Used Memory    Total Memory    Temperature\n",
            "----  --------  ------  -------------  -------------  --------------  -------------\n",
            "   0  Tesla T4  0.0%    11055.0MB      4893.0MB       16384.0MB       36.0 C\n",
            "CPU Utilization: 2.5%\n",
            "Memory Utilization: 20.8%\n",
            "  ID  Name      Load    Free Memory    Used Memory    Total Memory    Temperature\n",
            "----  --------  ------  -------------  -------------  --------------  -------------\n",
            "   0  Tesla T4  0.0%    11055.0MB      4893.0MB       16384.0MB       36.0 C\n",
            "CPU Utilization: 0.0%\n",
            "Memory Utilization: 20.8%\n",
            "  ID  Name      Load    Free Memory    Used Memory    Total Memory    Temperature\n",
            "----  --------  ------  -------------  -------------  --------------  -------------\n",
            "   0  Tesla T4  0.0%    11055.0MB      4893.0MB       16384.0MB       36.0 C\n",
            "CPU Utilization: 0.0%\n",
            "Memory Utilization: 20.8%\n",
            "  ID  Name      Load    Free Memory    Used Memory    Total Memory    Temperature\n",
            "----  --------  ------  -------------  -------------  --------------  -------------\n",
            "   0  Tesla T4  0.0%    11055.0MB      4893.0MB       16384.0MB       36.0 C\n",
            "CPU Utilization: 0.0%\n",
            "Memory Utilization: 20.8%\n",
            "  ID  Name      Load    Free Memory    Used Memory    Total Memory    Temperature\n",
            "----  --------  ------  -------------  -------------  --------------  -------------\n",
            "   0  Tesla T4  0.0%    11055.0MB      4893.0MB       16384.0MB       36.0 C\n",
            "CPU Utilization: 0.0%\n",
            "Memory Utilization: 20.8%\n",
            "  ID  Name      Load    Free Memory    Used Memory    Total Memory    Temperature\n",
            "----  --------  ------  -------------  -------------  --------------  -------------\n",
            "   0  Tesla T4  0.0%    11055.0MB      4893.0MB       16384.0MB       36.0 C\n",
            "CPU Utilization: 0.0%\n",
            "Memory Utilization: 20.8%\n",
            "  ID  Name      Load    Free Memory    Used Memory    Total Memory    Temperature\n",
            "----  --------  ------  -------------  -------------  --------------  -------------\n",
            "   0  Tesla T4  0.0%    11055.0MB      4893.0MB       16384.0MB       36.0 C\n",
            "CPU Utilization: 0.0%\n",
            "Memory Utilization: 20.8%\n",
            "  ID  Name      Load    Free Memory    Used Memory    Total Memory    Temperature\n",
            "----  --------  ------  -------------  -------------  --------------  -------------\n",
            "   0  Tesla T4  0.0%    11055.0MB      4893.0MB       16384.0MB       36.0 C\n",
            "CPU Utilization: 2.5%\n",
            "Memory Utilization: 20.8%\n",
            "  ID  Name      Load    Free Memory    Used Memory    Total Memory    Temperature\n",
            "----  --------  ------  -------------  -------------  --------------  -------------\n",
            "   0  Tesla T4  0.0%    11055.0MB      4893.0MB       16384.0MB       36.0 C\n",
            "CPU Utilization: 0.0%\n",
            "Memory Utilization: 20.8%\n",
            "  ID  Name      Load    Free Memory    Used Memory    Total Memory    Temperature\n",
            "----  --------  ------  -------------  -------------  --------------  -------------\n",
            "   0  Tesla T4  0.0%    11055.0MB      4893.0MB       16384.0MB       36.0 C\n",
            "CPU Utilization: 0.0%\n",
            "Memory Utilization: 20.8%\n",
            "  ID  Name      Load    Free Memory    Used Memory    Total Memory    Temperature\n",
            "----  --------  ------  -------------  -------------  --------------  -------------\n",
            "   0  Tesla T4  0.0%    11055.0MB      4893.0MB       16384.0MB       36.0 C\n",
            "CPU Utilization: 0.0%\n",
            "Memory Utilization: 20.8%\n",
            "  ID  Name      Load    Free Memory    Used Memory    Total Memory    Temperature\n",
            "----  --------  ------  -------------  -------------  --------------  -------------\n",
            "   0  Tesla T4  0.0%    11055.0MB      4893.0MB       16384.0MB       36.0 C\n",
            "CPU Utilization: 0.0%\n",
            "Memory Utilization: 20.8%\n",
            "  ID  Name      Load    Free Memory    Used Memory    Total Memory    Temperature\n",
            "----  --------  ------  -------------  -------------  --------------  -------------\n",
            "   0  Tesla T4  0.0%    11055.0MB      4893.0MB       16384.0MB       36.0 C\n",
            "CPU Utilization: 0.0%\n",
            "Memory Utilization: 20.8%\n",
            "  ID  Name      Load    Free Memory    Used Memory    Total Memory    Temperature\n",
            "----  --------  ------  -------------  -------------  --------------  -------------\n",
            "   0  Tesla T4  0.0%    11055.0MB      4893.0MB       16384.0MB       36.0 C\n",
            "CPU Utilization: 2.5%\n",
            "Memory Utilization: 20.8%\n",
            "  ID  Name      Load    Free Memory    Used Memory    Total Memory    Temperature\n",
            "----  --------  ------  -------------  -------------  --------------  -------------\n",
            "   0  Tesla T4  0.0%    11055.0MB      4893.0MB       16384.0MB       36.0 C\n",
            "CPU Utilization: 2.4%\n",
            "Memory Utilization: 20.8%\n",
            "  ID  Name      Load    Free Memory    Used Memory    Total Memory    Temperature\n",
            "----  --------  ------  -------------  -------------  --------------  -------------\n",
            "   0  Tesla T4  0.0%    11055.0MB      4893.0MB       16384.0MB       36.0 C\n",
            "CPU Utilization: 0.0%\n",
            "Memory Utilization: 20.8%\n",
            "  ID  Name      Load    Free Memory    Used Memory    Total Memory    Temperature\n",
            "----  --------  ------  -------------  -------------  --------------  -------------\n",
            "   0  Tesla T4  0.0%    11055.0MB      4893.0MB       16384.0MB       36.0 C\n",
            "CPU Utilization: 0.0%\n",
            "Memory Utilization: 20.8%\n",
            "  ID  Name      Load    Free Memory    Used Memory    Total Memory    Temperature\n",
            "----  --------  ------  -------------  -------------  --------------  -------------\n",
            "   0  Tesla T4  0.0%    11055.0MB      4893.0MB       16384.0MB       36.0 C\n",
            "CPU Utilization: 0.0%\n",
            "Memory Utilization: 20.8%\n",
            "  ID  Name      Load    Free Memory    Used Memory    Total Memory    Temperature\n",
            "----  --------  ------  -------------  -------------  --------------  -------------\n",
            "   0  Tesla T4  0.0%    11055.0MB      4893.0MB       16384.0MB       36.0 C\n",
            "CPU Utilization: 0.0%\n",
            "Memory Utilization: 20.8%\n",
            "  ID  Name      Load    Free Memory    Used Memory    Total Memory    Temperature\n",
            "----  --------  ------  -------------  -------------  --------------  -------------\n",
            "   0  Tesla T4  0.0%    11055.0MB      4893.0MB       16384.0MB       36.0 C\n",
            "CPU Utilization: 0.0%\n",
            "Memory Utilization: 20.8%\n",
            "  ID  Name      Load    Free Memory    Used Memory    Total Memory    Temperature\n",
            "----  --------  ------  -------------  -------------  --------------  -------------\n",
            "   0  Tesla T4  0.0%    11055.0MB      4893.0MB       16384.0MB       36.0 C\n",
            "CPU Utilization: 0.0%\n",
            "Memory Utilization: 20.8%\n",
            "  ID  Name      Load    Free Memory    Used Memory    Total Memory    Temperature\n",
            "----  --------  ------  -------------  -------------  --------------  -------------\n",
            "   0  Tesla T4  0.0%    11055.0MB      4893.0MB       16384.0MB       36.0 C\n",
            "CPU Utilization: 0.0%\n",
            "Memory Utilization: 20.8%\n",
            "  ID  Name      Load    Free Memory    Used Memory    Total Memory    Temperature\n",
            "----  --------  ------  -------------  -------------  --------------  -------------\n",
            "   0  Tesla T4  0.0%    11055.0MB      4893.0MB       16384.0MB       36.0 C\n",
            "CPU Utilization: 0.0%\n",
            "Memory Utilization: 20.8%\n",
            "  ID  Name      Load    Free Memory    Used Memory    Total Memory    Temperature\n",
            "----  --------  ------  -------------  -------------  --------------  -------------\n",
            "   0  Tesla T4  0.0%    11055.0MB      4893.0MB       16384.0MB       36.0 C\n",
            "CPU Utilization: 0.0%\n",
            "Memory Utilization: 20.8%\n",
            "  ID  Name      Load    Free Memory    Used Memory    Total Memory    Temperature\n",
            "----  --------  ------  -------------  -------------  --------------  -------------\n",
            "   0  Tesla T4  0.0%    11055.0MB      4893.0MB       16384.0MB       36.0 C\n",
            "CPU Utilization: 0.0%\n",
            "Memory Utilization: 20.8%\n",
            "  ID  Name      Load    Free Memory    Used Memory    Total Memory    Temperature\n",
            "----  --------  ------  -------------  -------------  --------------  -------------\n",
            "   0  Tesla T4  0.0%    11055.0MB      4893.0MB       16384.0MB       36.0 C\n",
            "CPU Utilization: 0.0%\n",
            "Memory Utilization: 20.8%\n",
            "  ID  Name      Load    Free Memory    Used Memory    Total Memory    Temperature\n",
            "----  --------  ------  -------------  -------------  --------------  -------------\n",
            "   0  Tesla T4  0.0%    11055.0MB      4893.0MB       16384.0MB       36.0 C\n",
            "CPU Utilization: 0.0%\n",
            "Memory Utilization: 20.8%\n",
            "  ID  Name      Load    Free Memory    Used Memory    Total Memory    Temperature\n",
            "----  --------  ------  -------------  -------------  --------------  -------------\n",
            "   0  Tesla T4  0.0%    11055.0MB      4893.0MB       16384.0MB       36.0 C\n",
            "CPU Utilization: 0.0%\n",
            "Memory Utilization: 20.8%\n",
            "  ID  Name      Load    Free Memory    Used Memory    Total Memory    Temperature\n",
            "----  --------  ------  -------------  -------------  --------------  -------------\n",
            "   0  Tesla T4  0.0%    11055.0MB      4893.0MB       16384.0MB       36.0 C\n",
            "CPU Utilization: 0.0%\n",
            "Memory Utilization: 20.8%\n",
            "  ID  Name      Load    Free Memory    Used Memory    Total Memory    Temperature\n",
            "----  --------  ------  -------------  -------------  --------------  -------------\n",
            "   0  Tesla T4  0.0%    11055.0MB      4893.0MB       16384.0MB       36.0 C\n",
            "CPU Utilization: 0.0%\n",
            "Memory Utilization: 20.8%\n",
            "  ID  Name      Load    Free Memory    Used Memory    Total Memory    Temperature\n",
            "----  --------  ------  -------------  -------------  --------------  -------------\n",
            "   0  Tesla T4  0.0%    11055.0MB      4893.0MB       16384.0MB       36.0 C\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[17], line 40\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo GPU detected.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     38\u001b[0m         time\u001b[38;5;241m.\u001b[39msleep(interval)\n\u001b[0;32m---> 40\u001b[0m \u001b[43mprint_gpu_cpu_usage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[17], line 38\u001b[0m, in \u001b[0;36mprint_gpu_cpu_usage\u001b[0;34m(interval)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo GPU detected.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 38\u001b[0m \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43minterval\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "#hardware monitoring\n",
        "%pip install psutil gputil\n",
        "import psutil\n",
        "import GPUtil\n",
        "from tabulate import tabulate\n",
        "import time\n",
        "def print_gpu_cpu_usage(interval=1):\n",
        "    while True:\n",
        "        # Obtenir l'utilisation du CPU\n",
        "        cpu_percent = psutil.cpu_percent(interval=0.1)\n",
        "\n",
        "        # Obtenir l'utilisation de la mémoire\n",
        "        memory_info = psutil.virtual_memory()\n",
        "        memory_percent = memory_info.percent\n",
        "\n",
        "        # Obtenir l'utilisation du GPU\n",
        "        gpus = GPUtil.getGPUs()\n",
        "        list_gpus = []\n",
        "        for gpu in gpus:\n",
        "            list_gpus.append((\n",
        "                gpu.id,\n",
        "                f\"{gpu.name}\",\n",
        "                f\"{gpu.load*100}%\",\n",
        "                f\"{gpu.memoryFree}MB\",\n",
        "                f\"{gpu.memoryUsed}MB\",\n",
        "                f\"{gpu.memoryTotal}MB\",\n",
        "                f\"{gpu.temperature} C\"\n",
        "            ))\n",
        "\n",
        "        # Afficher les informations de manière tabulée\n",
        "        print(f\"CPU Utilization: {cpu_percent}%\")\n",
        "        print(f\"Memory Utilization: {memory_percent}%\")\n",
        "        if list_gpus:\n",
        "            print(tabulate(list_gpus, headers=(\"ID\", \"Name\", \"Load\", \"Free Memory\", \"Used Memory\", \"Total Memory\", \"Temperature\")))\n",
        "        else:\n",
        "            print(\"No GPU detected.\")\n",
        "        \n",
        "        time.sleep(interval)\n",
        "\n",
        "print_gpu_cpu_usage()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Thu Jan 23 22:24:06 2025       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.216.03             Driver Version: 535.216.03   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       On  | 00000001:00:00.0 Off |                  Off |\n",
            "| N/A   38C    P0              26W /  70W |  15615MiB / 16384MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|    0   N/A  N/A      3938      C   ...envs/azureml_py310_sdkv2/bin/python    15610MiB |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        }
      ],
      "source": [
        "#clean up GPU memory if GPU out of memory\n",
        "#torch.cuda.empty_cache()\n",
        "\n",
        "#show the ressource usage\n",
        "#!nvidia-smi\n",
        "\n",
        "#if other problem, restart the kernel or try the following command\n",
        "#Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false) to enable or disable the parallelism in the tokenizers library.\n",
        "#import os\n",
        "#os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
        "#model.config.quantization_config.to_dict()\n",
        "\n",
        "#os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128'\n",
        "#os.getenv('PYTORCH_CUDA_ALLOC_CONF')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Training part with dataset loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets import load_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading data: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1.18G/1.18G [01:02<00:00, 19.0MB/s]\n",
            "Generating train split: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 74004228/74004228 [17:05<00:00, 72179.97 examples/s]\n"
          ]
        }
      ],
      "source": [
        "#loading the bookcorpus dataset\n",
        "bookcorpus = load_dataset(\"bookcorpus\",trust_remote_code=True) \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['text'],\n",
              "        num_rows: 74004228\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bookcorpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Generating train split: 98161 examples [00:10, 9736.79 examples/s] \n"
          ]
        }
      ],
      "source": [
        "#loading the rocstories dataset\n",
        "roc = load_dataset(\"wza/roc_stories\",trust_remote_code=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['storyid', 'storytitle', 'sentence1', 'sentence2', 'sentence3', 'sentence4', 'sentence5'],\n",
              "        num_rows: 98161\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "roc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Example of sentence in train_dataset=  {'text': 'but just one look at a minion sent him practically catatonic .'}\n"
          ]
        }
      ],
      "source": [
        "train_dataset = bookcorpus[\"train\"]\n",
        "print(f'Example of sentence in train_dataset= ',train_dataset[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Example of sentence in train_dataset=  {'storyid': '617e7ada-3878-488d-bd56-40695b91f053', 'storytitle': 'The Bike Accident', 'sentence1': 'Carrie had just learned how to ride a bike.', 'sentence2': \"She didn't have a bike of her own.\", 'sentence3': \"Carrie would sneak rides on her sister's bike.\", 'sentence4': 'She got nervous on a hill and crashed into a wall.', 'sentence5': 'The bike frame bent and Carrie got a deep gash on her leg.'}\n"
          ]
        }
      ],
      "source": [
        "train_dataset = roc[\"train\"]\n",
        "print(f'Example of sentence in train_dataset= ',train_dataset[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments, Trainer, TrainerCallback\n",
        "from evaluate import load"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting rouge_score\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25hCollecting nltk\n",
            "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting absl-py (from rouge_score)\n",
            "  Downloading absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: numpy in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from rouge_score) (1.23.5)\n",
            "Requirement already satisfied: six>=1.14.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from rouge_score) (1.16.0)\n",
            "Requirement already satisfied: click in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from nltk) (4.66.5)\n",
            "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m33.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
            "Building wheels for collected packages: rouge_score\n",
            "  Building wheel for rouge_score (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24935 sha256=aee4cf37725a66b7d2b4a2077ebb176d9ba01e50e2a51c19c810f2aa1d6e4e1a\n",
            "  Stored in directory: /home/azureuser/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n",
            "Successfully built rouge_score\n",
            "Installing collected packages: nltk, absl-py, rouge_score\n",
            "Successfully installed absl-py-2.1.0 nltk-3.9.1 rouge_score-0.1.2\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "#evaluate the model with rouge score\n",
        "%pip install rouge_score nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Map:   0%|          | 0/98161 [00:00<?, ? examples/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Map: 100%|██████████| 98161/98161 [00:34<00:00, 2843.13 examples/s]\n"
          ]
        }
      ],
      "source": [
        "# Preprocess the dataset with the tokenizer before training\n",
        "def tokenize_bc_function(examples):\n",
        "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=512)\n",
        "\n",
        "def tokenize_roc_function(examples):\n",
        "    return tokenizer(examples[\"sentence1\"],examples[\"sentence2\"],examples[\"sentence3\"],examples[\"sentence4\"], padding=\"max_length\", truncation=True, max_length=512)\n",
        "\n",
        "tokenized_datasets = roc.map(tokenize_roc_function, batched=True) \n",
        "#tokenized_datasets1 = bookcorpus.map(tokenize_bc_function, batched=True)\n",
        "\n",
        "tokenized_datasets = tokenized_datasets.remove_columns(['storyid', 'storytitle', 'sentence1', 'sentence2', 'sentence3', 'sentence4', 'sentence5'])\n",
        "#tokenized_datasets1 = tokenized_datasets1.remove_columns([\"text\"])\n",
        "\n",
        "tokenized_datasets.set_format(\"torch\")\n",
        "#tokenized_datasets1.set_format(\"torch\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [],
      "source": [
        "#split the tokenized_datasets mapping with dataset into training and validation (90% training and 10% validation)\n",
        "tokenized_datasets = tokenized_datasets[\"train\"].train_test_split(test_size=0.2)\n",
        "tokenized_datasets[\"validation\"]=tokenized_datasets[\"test\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['input_ids', 'attention_mask', 'labels'],\n",
              "        num_rows: 63607\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['input_ids', 'attention_mask', 'labels'],\n",
              "        num_rows: 15902\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['input_ids', 'attention_mask', 'labels'],\n",
              "        num_rows: 15902\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenized_datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading builder script: 100%|██████████| 7.02k/7.02k [00:00<00:00, 18.7MB/s]\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     /home/azureuser/nltk_data...\n",
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     /home/azureuser/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package omw-1.4 to\n",
            "[nltk_data]     /home/azureuser/nltk_data...\n",
            "Downloading builder script: 100%|██████████| 5.94k/5.94k [00:00<00:00, 16.9MB/s]\n",
            "Downloading extra modules: 4.07kB [00:00, 10.5MB/s]                   \n",
            "Downloading extra modules: 100%|██████████| 3.34k/3.34k [00:00<00:00, 20.1MB/s]\n",
            "Downloading builder script: 100%|██████████| 4.20k/4.20k [00:00<00:00, 21.4MB/s]\n"
          ]
        }
      ],
      "source": [
        "# metrics lodaded from the evaluate library\n",
        "rouge = load(\"rouge\")\n",
        "meteor = load(\"meteor\")\n",
        "bleu = load(\"bleu\")\n",
        "accuracy = load(\"accuracy\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "    \n",
        "    rouge_result = rouge.compute(predictions=decoded_preds, references=decoded_labels)\n",
        "    meteor_result = meteor.compute(predictions=decoded_preds, references=decoded_labels)\n",
        "    bleu_result = bleu.compute(predictions=decoded_preds, references=decoded_labels)\n",
        "    accuracy_result = accuracy.compute(predictions=decoded_preds, references=decoded_labels)\n",
        "    \n",
        "    return {\n",
        "        \"rouge\": rouge_result,\n",
        "        \"meteor\": meteor_result,\n",
        "        \"bleu\": bleu_result,\n",
        "        \"accuracy\": accuracy_result,\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install peft"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### [Some docs about peft fine-tuning methods](https://huggingface.co/docs/transformers/peft)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [
        {
          "ename": "KeyError",
          "evalue": "'adapter'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[55], line 10\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpeft\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PeftModel, PeftConfig\n\u001b[1;32m      4\u001b[0m peft_config \u001b[38;5;241m=\u001b[39m PeftConfig(\n\u001b[1;32m      5\u001b[0m     task_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCAUSAL_LM\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      6\u001b[0m     peft_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madapter\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      7\u001b[0m     auto_mapping\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      8\u001b[0m     base_model_name_or_path\u001b[38;5;241m=\u001b[39mmodel_name,\n\u001b[1;32m      9\u001b[0m )\n\u001b[0;32m---> 10\u001b[0m model_with_adapters \u001b[38;5;241m=\u001b[39m \u001b[43mPeftModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m#or if use directly the model with adaptaters\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m#model.add_adapter(peft_config)\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m#enable or disable adaptaters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     16\u001b[0m \n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Config for the training with the Trainer API\u001b[39;00m\n\u001b[1;32m     18\u001b[0m training_args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[1;32m     19\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./results\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     20\u001b[0m     evaluation_strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     28\u001b[0m     eval_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m,\n\u001b[1;32m     29\u001b[0m )\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/peft/peft_model.py:173\u001b[0m, in \u001b[0;36mPeftModel.__init__\u001b[0;34m(self, model, peft_config, adapter_name, autocast_adapter_dtype, low_cpu_mem_usage)\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    172\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_peft_config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mPEFT_TYPE_TO_MODEL_MAPPING\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpeft_type\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    174\u001b[0m     ctx \u001b[38;5;241m=\u001b[39m init_empty_weights \u001b[38;5;28;01mif\u001b[39;00m low_cpu_mem_usage \u001b[38;5;28;01melse\u001b[39;00m nullcontext\n\u001b[1;32m    175\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx():\n",
            "\u001b[0;31mKeyError\u001b[0m: 'adapter'"
          ]
        }
      ],
      "source": [
        "# Remarks: some error in the code, the model is not trained\n",
        "#for training the model the GPTQ model requires some configuration using peft\n",
        "from peft import PeftModel, PeftConfig\n",
        "\n",
        "peft_config = PeftConfig(\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    peft_type=\"adapter\",\n",
        "    auto_mapping=True,\n",
        "    base_model_name_or_path=model_name,\n",
        ")\n",
        "model_with_adapters = PeftModel(model, peft_config)\n",
        "#or if use directly the model with adaptaters\n",
        "#model.add_adapter(peft_config)\n",
        "#enable or disable adaptaters\n",
        "#model.enable_adapters()\n",
        "#model.disable_adapters()\n",
        "\n",
        "# Config for the training with the Trainer API\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=10,\n",
        "    save_total_limit=2,\n",
        "    save_steps=500,\n",
        "    eval_steps=500,\n",
        ")\n",
        "\n",
        "# Callback to print the metrics during training\n",
        "class LogMetricsCallback(TrainerCallback):\n",
        "    def on_evaluate(self, args, state, control, metrics=None, **kwargs):\n",
        "        if metrics is not None:\n",
        "            print(f\"Metrics at step {state.global_step}: {metrics}\")\n",
        "            \n",
        "    def on_train_begin(self, args, state, control, **kwargs):\n",
        "        print(\"Training is starting\")\n",
        "\n",
        "    def on_train_end(self, args, state, control, **kwargs):\n",
        "        print(\"Training is finished\")\n",
        "\n",
        "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
        "        print(f\"Step: {state.global_step}, Loss: {logs['loss']}\")\n",
        "\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model_with_adapters,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],\n",
        "    compute_metrics=compute_metrics,\n",
        "    callbacks=[LogMetricsCallback()],\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "#save the model\n",
        "save_dir = \"roc_trained_model\"\n",
        "model_with_adapters.save_pretrained(save_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Model evaluation with metrics\n",
        "### [Cf this link to Hugging Face about evaluation model](https://huggingface.co/evaluate-metric)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     /home/azureuser/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     /home/azureuser/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to\n",
            "[nltk_data]     /home/azureuser/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "from evaluate import load\n",
        "#model.eval()\n",
        "\n",
        "# Setup evaluation \n",
        "metric = load(\"rouge\")\n",
        "metric2 = load(\"meteor\")\n",
        "# metric3 = load(\"accuracy\")\n",
        "# metric4 = load(\"bleu\")\n",
        "# metric5 = load(\"f1\")\n",
        "# metric6 = load(\"precision\")\n",
        "# metric7 = load(\"recall\")\n",
        "\n",
        "\n",
        "def evaluate_model(prompts, references,max_tokens=100,metric=metric):\n",
        "    generated_texts = [generate_text(prompt,max_new_tokens=max_tokens) for prompt in prompts]\n",
        "    print(f'Prediction= {[generated_text for generated_text in generated_texts]}\\n')\n",
        "    results = metric.compute(predictions=generated_texts, references=references)\n",
        "    return results\n",
        "\n",
        "def rouge_metric(generated_text, reference):\n",
        "    return metric.compute(predictions=[generated_text], references=[reference])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prompts= The neverending day began with a beautiful sunshine and an AI robot which was seeking humans on the desert Earth.\n",
            "\n",
            "Prediction= ['The neverending day began with a beautiful sunshine and an AI robot which was seeking humans on the desert Earth. Its mission was simple: to locate the remaining humans and take them to a safe place. The AI robot was programmed to avoid the hazardous areas that were filled with mutated creatures, but it encountered a group of humans who were fighting against the mutated creatures.\\n\\nThe AI robot immediately identified that the group was a potential threat to its mission, so it tried to communicate with them. The group, however, was hesitant and refused to cooperate with the robot. The']\n",
            "\n",
            "References= [\"As the sun rose on the barren wasteland that once was Earth, an AI robot named X-979 set out on its daily mission. The robot's programming had been designed to search for signs of human life, a task that had proved futile for many years.\"]\n",
            "\n",
            "Rouge_score = {'meteor': 0.2721283422459893}\n"
          ]
        }
      ],
      "source": [
        "#evaluation of the pretrained model without additional training\n",
        "prompts = [\"The neverending day began with a beautiful sunshine and an AI robot which was seeking humans on the desert Earth.\"]\n",
        "references = [\"As the sun rose on the barren wasteland that once was Earth, an AI robot named X-979 set out on its daily mission. The robot's programming had been designed to search for signs of human life, a task that had proved futile for many years.\"]\n",
        "\n",
        "print(f'Prompts= {prompts[0]}\\n')\n",
        "results = evaluate_model(prompts, references,metric=metric2)\n",
        "print(f'References= {[reference for reference in references]}\\n')\n",
        "print(f'Meteor_score = {results}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Prompts= The neverending day began with a beautiful sunshine and an AI robot which was seeking humans on the desert Earth.\n",
        "\n",
        "Prediction= ['The neverending day began with a beautiful sunshine and an AI robot which was seeking humans on the desert Earth. Its mission was simple: to locate the remaining humans and take them to a safe place. The AI robot was programmed to avoid the hazardous areas that were filled with mutated creatures, but it encountered a group of humans who were fighting against the mutated creatures.\\n\\nThe AI robot immediately identified that the group was a potential threat to its mission, so it tried to communicate with them. The group, however, was hesitant and refused to cooperate with the robot. The']\n",
        "\n",
        "References= [\"As the sun rose on the barren wasteland that once was Earth, an AI robot named X-979 set out on its daily mission. The robot's programming had been designed to search for signs of human life, a task that had proved futile for many years.\"]\n",
        "\n",
        "Meteor_score = {'meteor': 0.2721283422459893}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prompts= The neverending day began with a beautiful sunshine and an AI robot which was seeking humans on the desert Earth.\n",
            "\n",
            "Prediction= [\"The neverending day began with a beautiful sunshine and an AI robot which was seeking humans on the desert Earth. I was the only human that I have found, and the AI robot was happy to find me.\\n\\nAfter that, I became a part of his journey. It was hard to say how long we have been traveling together. It might be a few days, weeks, or even months. Time had lost its meaning on the barren and lifeless planet.\\n\\nThe AI robot's name was X-113, and it had been designed by humans to be a part of\"]\n",
            "\n",
            "References= [\"As the sun rose on the barren wasteland that once was Earth, an AI robot named X-979 set out on its daily mission. The robot's programming had been designed to search for signs of human life, a task that had proved futile for many years.\"]\n",
            "\n",
            "Rouge_score = {'rouge1': 0.3401360544217687, 'rouge2': 0.09655172413793103, 'rougeL': 0.2040816326530612, 'rougeLsum': 0.2448979591836734}\n"
          ]
        }
      ],
      "source": [
        "prompts = [\"The neverending day began with a beautiful sunshine and an AI robot which was seeking humans on the desert Earth.\"]\n",
        "references = [\"As the sun rose on the barren wasteland that once was Earth, an AI robot named X-979 set out on its daily mission. The robot's programming had been designed to search for signs of human life, a task that had proved futile for many years.\"]\n",
        "#prompts = [\"Once upon a time an ice-cream met a spoon and they fell in love\"]\n",
        "#references = [\"Once upon a time, there was an ice-cream named Charlie who was scooped from the freezer and placed in a cup. Charlie was a friendly and outgoing ice-cream who loved making friends. As soon as Charlie saw a spoon named Sarah, he knew he had found his soulmate.\"]\n",
        "\n",
        "print(f'Prompts= {prompts[0]}\\n')\n",
        "results = evaluate_model(prompts, references)\n",
        "print(f'References= {[reference for reference in references]}\\n')\n",
        "print(f'Rouge_score = {results}')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Prompts= The neverending day began with a beautiful sunshine and an AI robot which was seeking humans on the desert Earth.\n",
        "\n",
        "Prediction= [\"The neverending day began with a beautiful sunshine and an AI robot which was seeking humans on the desert Earth. I was the only human that I have found, and the AI robot was happy to find me.\\n\\nAfter that, I became a part of his journey. It was hard to say how long we have been traveling together. It might be a few days, weeks, or even months. Time had lost its meaning on the barren and lifeless planet.\\n\\nThe AI robot's name was X-113, and it had been designed by humans to be a part of\"]\n",
        "\n",
        "References= [\"As the sun rose on the barren wasteland that once was Earth, an AI robot named X-979 set out on its daily mission. The robot's programming had been designed to search for signs of human life, a task that had proved futile for many years.\"]\n",
        "\n",
        "Rouge_score = {'rouge1': 0.3401360544217687, 'rouge2': 0.09655172413793103, 'rougeL': 0.2040816326530612, 'rougeLsum': 0.2448979591836734}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Display the Gradio app in the notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting gradio\n",
            "  Downloading gradio-5.13.1-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting aiofiles<24.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting anyio<5.0,>=3.0 (from gradio)\n",
            "  Downloading anyio-4.8.0-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting fastapi<1.0,>=0.115.2 (from gradio)\n",
            "  Downloading fastapi-0.115.7-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.5.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting gradio-client==1.6.0 (from gradio)\n",
            "  Downloading gradio_client-1.6.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting httpx>=0.24.1 (from gradio)\n",
            "  Downloading httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: huggingface-hub>=0.25.1 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from gradio) (0.27.1)\n",
            "Requirement already satisfied: jinja2<4.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from gradio) (3.1.4)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from gradio) (2.1.5)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from gradio) (1.23.5)\n",
            "Collecting orjson~=3.0 (from gradio)\n",
            "  Downloading orjson-3.10.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (41 kB)\n",
            "Requirement already satisfied: packaging in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from gradio) (24.1)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from gradio) (1.3.5)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from gradio) (10.4.0)\n",
            "Collecting pydantic>=2.0 (from gradio)\n",
            "  Downloading pydantic-2.10.6-py3-none-any.whl.metadata (30 kB)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting python-multipart>=0.0.18 (from gradio)\n",
            "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from gradio) (6.0.2)\n",
            "Collecting ruff>=0.2.2 (from gradio)\n",
            "  Downloading ruff-0.9.3-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Collecting safehttpx<0.2.0,>=0.1.6 (from gradio)\n",
            "  Downloading safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
            "  Downloading starlette-0.45.3-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting tomlkit<0.14.0,>=0.12.0 (from gradio)\n",
            "  Downloading tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting typer<1.0,>=0.12 (from gradio)\n",
            "  Downloading typer-0.15.1-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from gradio) (4.12.2)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Downloading uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: fsspec in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from gradio-client==1.6.0->gradio) (2023.10.0)\n",
            "Collecting websockets<15.0,>=10.0 (from gradio-client==1.6.0->gradio)\n",
            "  Downloading websockets-14.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.2 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from anyio<5.0,>=3.0->gradio) (1.2.2)\n",
            "Requirement already satisfied: idna>=2.8 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Collecting sniffio>=1.1 (from anyio<5.0,>=3.0->gradio)\n",
            "  Downloading sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
            "Requirement already satisfied: certifi in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from httpx>=0.24.1->gradio) (2024.8.30)\n",
            "Collecting httpcore==1.* (from httpx>=0.24.1->gradio)\n",
            "  Downloading httpcore-1.0.7-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx>=0.24.1->gradio)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Requirement already satisfied: filelock in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from huggingface-hub>=0.25.1->gradio) (3.16.1)\n",
            "Requirement already satisfied: requests in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from huggingface-hub>=0.25.1->gradio) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from huggingface-hub>=0.25.1->gradio) (4.66.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from pandas<3.0,>=1.0->gradio) (2.9.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
            "Collecting annotated-types>=0.6.0 (from pydantic>=2.0->gradio)\n",
            "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting pydantic-core==2.27.2 (from pydantic>=2.0->gradio)\n",
            "  Downloading pydantic_core-2.27.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: click>=8.0.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\n",
            "Collecting shellingham>=1.3.0 (from typer<1.0,>=0.12->gradio)\n",
            "  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting rich>=10.11.0 (from typer<1.0,>=0.12->gradio)\n",
            "  Downloading rich-13.9.4-py3-none-any.whl.metadata (18 kB)\n",
            "Requirement already satisfied: six>=1.5 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from python-dateutil>=2.7.3->pandas<3.0,>=1.0->gradio) (1.16.0)\n",
            "Collecting markdown-it-py>=2.2.0 (from rich>=10.11.0->typer<1.0,>=0.12->gradio)\n",
            "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from requests->huggingface-hub>=0.25.1->gradio) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from requests->huggingface-hub>=0.25.1->gradio) (2.2.3)\n",
            "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio)\n",
            "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
            "Downloading gradio-5.13.1-py3-none-any.whl (57.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 MB\u001b[0m \u001b[31m48.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.6.0-py3-none-any.whl (321 kB)\n",
            "Downloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading anyio-4.8.0-py3-none-any.whl (96 kB)\n",
            "Downloading fastapi-0.115.7-py3-none-any.whl (94 kB)\n",
            "Downloading httpx-0.28.1-py3-none-any.whl (73 kB)\n",
            "Downloading httpcore-1.0.7-py3-none-any.whl (78 kB)\n",
            "Downloading orjson-3.10.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n",
            "Downloading pydantic-2.10.6-py3-none-any.whl (431 kB)\n",
            "Downloading pydantic_core-2.27.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m65.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
            "Downloading ruff-0.9.3-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m60.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
            "\u001b[?25hDownloading safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\n",
            "Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading starlette-0.45.3-py3-none-any.whl (71 kB)\n",
            "Downloading tomlkit-0.13.2-py3-none-any.whl (37 kB)\n",
            "Downloading typer-0.15.1-py3-none-any.whl (44 kB)\n",
            "Downloading uvicorn-0.34.0-py3-none-any.whl (62 kB)\n",
            "Downloading ffmpy-0.5.0-py3-none-any.whl (6.0 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
            "Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "Downloading rich-13.9.4-py3-none-any.whl (242 kB)\n",
            "Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
            "Downloading sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
            "Downloading websockets-14.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (169 kB)\n",
            "Downloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
            "Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
            "Installing collected packages: pydub, websockets, tomlkit, sniffio, shellingham, semantic-version, ruff, python-multipart, pydantic-core, orjson, mdurl, h11, ffmpy, annotated-types, aiofiles, uvicorn, pydantic, markdown-it-py, httpcore, anyio, starlette, rich, httpx, typer, safehttpx, gradio-client, fastapi, gradio\n",
            "Successfully installed aiofiles-23.2.1 annotated-types-0.7.0 anyio-4.8.0 fastapi-0.115.7 ffmpy-0.5.0 gradio-5.13.1 gradio-client-1.6.0 h11-0.14.0 httpcore-1.0.7 httpx-0.28.1 markdown-it-py-3.0.0 mdurl-0.1.2 orjson-3.10.15 pydantic-2.10.6 pydantic-core-2.27.2 pydub-0.25.1 python-multipart-0.0.20 rich-13.9.4 ruff-0.9.3 safehttpx-0.1.6 semantic-version-2.10.0 shellingham-1.5.4 sniffio-1.3.1 starlette-0.45.3 tomlkit-0.13.2 typer-0.15.1 uvicorn-0.34.0 websockets-14.2\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "* Running on local URL:  http://127.0.0.1:7860\n",
            "* Running on public URL: https://456f4509e5a857e701.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://456f4509e5a857e701.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Time to generate the story : 46.24886965751648 s\n",
            "max_tokens = 50\n",
            "temperature = 0.7\n",
            "top_p = 0.95\n",
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://456f4509e5a857e701.gradio.live\n"
          ]
        }
      ],
      "source": [
        "import gradio as gr\n",
        "import time\n",
        "from tqdm import tqdm \n",
        "import matplotlib.pyplot as plt\n",
        "#import threading  \n",
        "\n",
        "time_story = 0\n",
        "\n",
        "def generate_response(input,history: list[tuple[str, str]],max_tokens, temperature, top_p):\n",
        "    messages=[]\n",
        "    for val in history:\n",
        "        # Directly access content using \"content\" key\n",
        "        messages.extend([{\"role\": \"user\", \"content\": val.get(\"content\")}, {\"role\": \"assistant\", \"content\": val.get(\"content\")}]) if val else None\n",
        "\n",
        "    messages.append({\"role\": \"user\", \"content\": input})\n",
        "    start = time.time()\n",
        "    \n",
        "    output = generate_text(input,max_new_tokens=max_tokens, top_p=top_p, temperature=temperature)\n",
        "    end = time.time()\n",
        "    time_story= end-start\n",
        "    print(f'Time to generate the story : {time_story} s')\n",
        "\n",
        "    history.append((input,output))\n",
        "    print(f'max_tokens = {max_tokens}')\n",
        "    print(f'temperature = {temperature}')\n",
        "    print(f'top_p = {top_p}')\n",
        "    yield output\n",
        "\n",
        "#define the chatinterface\n",
        "title = \"TeLLMyStory\"\n",
        "description = \"A LLM for stories generation aiming the reinforcement of the controllability aspect\"\n",
        "theme = gr.Theme.from_hub(\"Yntec/HaleyCH_Theme_Yellow_Blue\")\n",
        "examples=[[\"Once upon a time a witch named Malefique was against the wedding of her daughter with the son of the king of the nearby kingdom.\"],\n",
        "        [\"Once upon a time an ice-cream met a spoon and they fell in love\"],\n",
        "        [\"The neverending day began with a beautiful sunshine and an AI robot which was seeking humans on the desert Earth.\"]]\n",
        "\n",
        "demo = gr.ChatInterface(\n",
        "      generate_response,\n",
        "      type=\"messages\",\n",
        "      title=title,\n",
        "      description=description,\n",
        "      theme=theme,\n",
        "      examples=examples,\n",
        "      additional_inputs=[\n",
        "          gr.Slider(minimum=1, maximum=2048, value=512, step=1, label=\"Max new tokens\"),\n",
        "          gr.Slider(minimum=0.1, maximum=4.0, value=0.7, step=0.1, label=\"Temperature\"),\n",
        "          gr.Slider(\n",
        "              minimum=0.1,\n",
        "              maximum=1.0,\n",
        "              value=0.95,\n",
        "              step=0.05,\n",
        "              label=\"Top-p (nucleus sampling)\",\n",
        "          ),\n",
        "      ],\n",
        "    \n",
        "    stop_btn=\"Stop\",\n",
        "    delete_cache=[60,60],\n",
        "    show_progress=\"full\",\n",
        "    save_history=True,\n",
        "  )\n",
        "\n",
        "#close the previous gradio app window in the notebook (else windows are added on the output cell)\n",
        "demo.close()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    demo.launch(share=True,debug=True)"
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "kernelspec": {
      "display_name": "Python 3.10 - SDK v2",
      "language": "python",
      "name": "python310-sdkv2"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "fr"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
